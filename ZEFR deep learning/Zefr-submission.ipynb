{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News 20 group dataset modelling using deep learning\n",
    "\n",
    "The libraries used for building the model are : \n",
    "<br>\n",
    "&ensp; 1) Keras deep learning library\n",
    "<br>\n",
    "&ensp; 2) Theano as computational backend \n",
    "<br>\n",
    "&ensp; 3) Intel distribution of python for efficient math kernel library\n",
    "<br>\n",
    "&ensp; 4) Gensim library for text mining and building word embeddings\n",
    "<br>\n",
    "&ensp; 5) nltk and nltk_rake library for tokenizing and text cleaning \n",
    "<br>\n",
    "&ensp; 6) sklearn library for metrics \n",
    "<br>\n",
    "&ensp; 7) scipy and numpy for scientific computations\n",
    "<br>\n",
    "\n",
    "System configuration used for training : \n",
    "<br>\n",
    "&ensp;      MEMORY : 16384MB RAM\n",
    "<br>\n",
    " &ensp;      CPU    : Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz (8 CPUs), ~3.6GHz\n",
    "<br>\n",
    "&ensp;       OS     : Windows 7\n",
    "<br>\n",
    " &ensp;      Addons : Intel Math kernel library\n",
    "<br>\n",
    "&ensp;       GPU    : No\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "<img src=\"architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Considerations\n",
    "&ensp;1) Traditional word frequency based approaches doesnot catpure the information about the context in which a particualr &ensp;word is used.\n",
    "<br>\n",
    "&ensp;2) So, in order to caputure the context based information about the word, I have used gensim to train the word2vector &ensp;matrix which obtains how one word is related to other words and creates a weight matrix of combination of each word with &ensp;every other word in the text corpus. \n",
    "<br>\n",
    "&ensp;3) The next design consideration was about the neural network architecture to chose and the type of model to use i.e &ensp;either discriminative or generative. \n",
    "<br>\n",
    "&ensp;4) In generative models, the assumption is the data coming from a prior stochasitic distribution. So the number of &ensp;parameters to learn becomes very high and as the corpus size increases the latent feature vector calculation becomes &ensp;intractible. This is one of the reasons not to use generative models like Restricted boltzmann machines which uses Markov &ensp;chains with markov property. \n",
    "<br>\n",
    "&ensp;5) Similar is the case with Deep beilef or graphical generative models as they use RBM in their initial layers. So, I tend &ensp;to proceed with generative modelling. \n",
    "<br>\n",
    "&ensp;6) Based on the scenario, since we are considering text classification and since each word has some context associated &ensp;with its previous and succeeding words, it would be better if we chose a sliding window model. \n",
    "<br>\n",
    "&ensp;7) This leaves me with option to chose either Convolutional networks or LSTM(Long short term memory models) a type of &ensp;recurrent network. \n",
    "<br>\n",
    "&ensp;8) The architecture of Recurrent neural network is as below which uses the feature at time T while training T+1. The &ensp;problem with such model is, if there are thousands of features to be trained, then the derivate at 1000th feature will &ensp;diminish to 0 or infinity and so the the Recurrent network will suffer with so called vanishing/exploding gradient problem. \n",
    "    <img src = \"recurrentnetwork.PNG\">\n",
    "<br>\n",
    "&ensp;9) To tackle this issue,we use LSTM model which uses previous states only based on some previous conditions if they were &ensp;proven to be true. This also reduces the computational time very much as compared to Recurrent networks as some weights &ensp;are set off. \n",
    "    <img src = \"lstm.png\">\n",
    "<br>\n",
    "&ensp;10) On the other hand, we have convolutional neural networks which can also function well. But convolutional network can &ensp;operate only on fixed length inputs. \n",
    "<br>\n",
    "&ensp;11) Since the text sentences are not typically of fixed length, we need to pad up all the input vectors to same size and &ensp;this increases the number of features. So, I chose LSTM over convolution. \n",
    "<br>\n",
    "&ensp;12) The metric chosen to optimize was categorical cross entropy which is basically a kind of misclassification metric over &ensp;categories. This is provided by thano. \n",
    "&ensp;13) The optimizer chosen was Adam optimizer which I usually use which tends to outperform others in majority of ocassions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Libraries used \n",
    "import os\n",
    "os.environ['KERAS_BAKCEND'] = 'theano'   # setting backend to theano\n",
    "os.environ['OMP_NUM_THREADS']='6'        # number of threads theano can use  \n",
    "os.environ['MKL_NUM_THREADS'] = '6'\n",
    "import io\n",
    "import theano\n",
    "import string\n",
    "theano.config.openmp = True\n",
    "from rake_nltk import Rake\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from rake_nltk import Rake\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import  Callback\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from tensorflow.contrib import learn\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Global declaration secion\n",
    "corpus_dict = {} \n",
    "vectorarray = []\n",
    "x_vector = []\n",
    "vocab_len = 200\n",
    "batch_size = 40\n",
    "n_epoch = 8\n",
    "y_train=[]\n",
    "testdatapath = r'C:\\Users\\bxt160230\\PycharmProjects\\Zefr\\20news-bydate-test'\n",
    "traindatapath = r'C:\\Users\\bxt160230\\PycharmProjects\\Zefr\\20news-bydate-train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Helper class that is used for logging the metrics per batch of training per epoch\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    def __init__(self, display):\n",
    "        self.seen = 0\n",
    "        self.display = display\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.seen += logs.get('size', 0)\n",
    "        if self.seen % self.display == 0:\n",
    "            # you can access loss, accuracy in self.params['metrics']\n",
    "            print ( \"- Batch Loss:\", self.seen ,self.params['metrics'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Metho description\n",
    "## Plots the summary of training / validation accuracy and loss for each epoch.\n",
    "\n",
    "def plot_charts(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('accuracy.png')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('loss.png')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method used for training the model. \n",
    "## Starts by building a sequntial model. \n",
    "## 1) First is the weights embedding model \n",
    "## 2) Second is LSTM model \n",
    "## 3) Drop out method used for regularization . Drops out if weights are less than 0.3 \n",
    "## 4) Dense model with sigmoid activation for classifying the labels.\n",
    "\n",
    "def train_model(x_train, y_train ,n_symbols , embedding_weights, input_length ,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=vocab_len, input_dim=n_symbols, mask_zero=True, weights=[embedding_weights], input_length=input_length))\n",
    "    model.add(LSTM(vocab_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    print('Compiling the Model...')\n",
    "    adam = Adam(lr= 0.01)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    print(\"Train...\")\n",
    "    out_batch = NBatchLogger(display= batch_size)\n",
    "    y_train = np.array(y_train)\n",
    "    history = model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epoch, shuffle = True , validation_split= 0.2,callbacks=[out_batch])\n",
    "    print(model.summary())\n",
    "    plot_charts(history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts showing how model accuracy changes over each epoch \n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img src = \"accuracy.png\">\n",
    "</td>\n",
    "<td>\n",
    "<img src = \"loss.png\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description\n",
    "## Removes the punctuation marks, uses RAKE (Rapid Keyword Extraction Algorithm) for processing the text \\\n",
    "## which provides ranked phrases and extracts the keywords. The stop words and digits in the text are removed. \n",
    "\n",
    "def clean_sentence(text):\n",
    "    stop = stopwords.words('english')\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    cleaned_text = text.translate(translator)\n",
    "    r= Rake()\n",
    "    r.extract_keywords_from_text(cleaned_text)\n",
    "    cleaned_text = ' '.join([x[1] for x in r.rank_list if x[0] > 5]).strip().lower()\n",
    "    cleaned_text = ' '.join([x for x in word_tokenize(cleaned_text) if x not in stop])\n",
    "    return ''.join([i for i in cleaned_text if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description\n",
    "## Makes the data get converted to indexes used in dictionary and returns a vector of indexes in sequence of words in sentence\n",
    "\n",
    "def parse_dataset(w2indx, data):\n",
    "    x_train_vec_wordembd = []\n",
    "    for item in x_vector:\n",
    "        txt = item.split()\n",
    "        new_txt = []\n",
    "        for word in txt:\n",
    "            try:\n",
    "                new_txt.append(w2indx[word])\n",
    "            except:\n",
    "                new_txt.append(0)\n",
    "        x_train_vec_wordembd.append(new_txt)\n",
    "    return x_train_vec_wordembd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description \n",
    "##  1 Creates a word to index mapping\n",
    "##  2- Creates a word to vector mapping from the word embeddings \n",
    "##  3- Transforms the Training and Testing Dictionaries and retruns the and constructs a bag of words with pretrained weights.\n",
    "\n",
    "def create_dictionaries(model, x_vector):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}\n",
    "    return w2indx, w2vec , parse_dataset(w2indx,x_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description\n",
    "## Function that gets the filename of each file in every directory, and converts multiple lines of text to single line \n",
    "## Calls clean text and returns the output of clean text to caller function\n",
    "\n",
    "def process_content(page_content):\n",
    "    single_sentence = \"\"\n",
    "    with io.open(page_content) as f:\n",
    "        next(line for line in f if line.isspace())\n",
    "        for text in f:\n",
    "            if not text.isspace():\n",
    "                single_sentence = single_sentence + text.strip() + ' '\n",
    "    return clean_sentence(single_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description\n",
    "## This function takes each file from the directory of train and calls the process_content function defined above. \n",
    "## Creates X and Y vectors , performs random shuffle on X and Y and returns back x , y , number of classes to main function. \n",
    "\n",
    "def process_data(traindatapath, type):\n",
    "    y_vector = []\n",
    "    num_files_processed = 0\n",
    "    filenames = []\n",
    "    directories = os.listdir(traindatapath)\n",
    "    class_number = 0\n",
    "    per_directory_count = 0\n",
    "    for sub_dir in directories:\n",
    "        files = os.listdir(os.path.join(traindatapath,sub_dir))\n",
    "        for file in files:\n",
    "            num_files_processed += 1\n",
    "            filename = os.path.join(os.path.join(traindatapath,sub_dir),file)\n",
    "            filenames.append(filename)\n",
    "            cleaned_sentence = process_content(filename)\n",
    "            corpus_dict[filename] = cleaned_sentence\n",
    "            x_vector.append(cleaned_sentence)\n",
    "            y_vector.append(class_number)\n",
    "        class_number += 1\n",
    "    print(\"number of files processed is\" , per_directory_count)\n",
    "    combined = list(zip(x_vector, y_vector))\n",
    "    random.shuffle(combined)\n",
    "    x_vector[:], y_vector[:] = zip(*combined)\n",
    "    if(type == 'train'):\n",
    "        y_vector = np_utils.to_categorical(y_vector)\n",
    "        return x_vector , y_vector , class_number\n",
    "    else:\n",
    "        return x_vector, y_vector, class_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description \n",
    "## Uses the word2vec of gensim library. The function of word2vec is to build a word emedding. \n",
    "## The sliding window size is 5, which means ,for 5 words in sequence, the weights of embedding layer are trained \n",
    "## such that the context of the word is not lost. It prepares weights for each word and it is present in word2vector. \n",
    "\n",
    "def word2vec_construct():\n",
    "    max_sent_length = 0\n",
    "    print(\"called word to vec construct\")\n",
    "    word2vector = w2v.Word2Vec(size= vocab_len, min_count= 5, window=5 , sample=1e-3, iter = 2)\n",
    "    sentences = []\n",
    "    for line in x_vector:\n",
    "        sentences.append(word_tokenize(line))\n",
    "        max_sent_length = max(max_sent_length,len(word_tokenize(line)))\n",
    "    print(\"length of sentences in vocab is \",max_sent_length )\n",
    "    word2vector.build_vocab(sentences)\n",
    "    print(\"Word2Vec vocabulary length:\", len(word2vector.wv.vocab))\n",
    "    word2vector.train(sentences, total_examples=word2vector.corpus_count ,epochs= 2)\n",
    "    word2vector.save(\"word2vector.w2v\")\n",
    "    return word2vector , max_sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method description \n",
    "## Driver function\n",
    "\n",
    "def main():\n",
    "    training_filenames , labels ,x_train_vector , y_train_vector , num_classes = process_data(traindatapath , 'train')\n",
    "    print(\"shapes of x and y vectors \" ,len(x_train_vector),len(y_train_vector))\n",
    "    word2vector , max_sent_length = word2vec_construct()\n",
    "    index_dict, word_vectors , x_train_vec_wordembd = create_dictionaries(word2vector,x_train_vector )\n",
    "    print(\"maximum sentence length is \" , max_sent_length)\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    ## code for embedding weights with word vectors \n",
    "    embedding_weights = np.zeros((n_symbols,vocab_len))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    #print(embedding_weights)\n",
    "    #x_train_vec_wordembd = sequence.pad_sequences(x_train_vec_wordembd, maxlen= max_sent_length)\n",
    "    ## calling train function \n",
    "    model = train_model(x_train_vec_wordembd,y_train_vector , n_symbols, embedding_weights, max_sent_length ,num_classes)\n",
    "    model.save('my_model.h5') ## Saving the model \n",
    "    ## code for performing testing and prediction using the model \n",
    "    testing_filenames, labels, x_test_vector, y_test_vector , num_classes = get_filenames(testdatapath , 'test')\n",
    "    print(\"shapes of x and y vectors \", len(x_test_vector), len(y_test_vector))\n",
    "    index_dict, word_vectors, x_test_vec_wordembd = create_dictionaries(word2vector, x_test_vector)\n",
    "    #x_test_vec_wordembd = sequence.pad_sequences(x_test_vec_wordembd, maxlen=max_sent_length)\n",
    "    y_pred = model.predict_proba(x_test_vec_wordembd)\n",
    "    y_results = np.argmax(y_pred, axis=1)\n",
    "    ## prints the confusion matrix. \n",
    "    print(classification_report(y_test_vector, y_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output window  \n",
    "<br>\n",
    "shapes of x and y vectors  11314 11314\n",
    "<br>\n",
    "called word to vec construct\n",
    "<br>\n",
    "length of sentences in vocab is  13973\n",
    "<br>\n",
    "Word2Vec vocabulary length: 19953\n",
    "<br>\n",
    "maximum sentence length is  13973\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 13973, 200)        3990880   \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 200)               320800    \n",
    "________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 200)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 20)                4020      \n",
    "_________________________________________________________________\n",
    "<br>\n",
    "Total params: 4,315,620\n",
    "<br>\n",
    "Trainable params: 4,315,620\n",
    "<br>\n",
    "Non-trainable params: 0\n",
    "<br>\n",
    "Compiling the Model...\n",
    "<br>\n",
    "Train...\n",
    "<br>\n",
    "Train on 9051 samples, validate on 2263 samples\n",
    "<br>\n",
    "Summarized the output . Actual output would be to print the metrics for each batch iteration\n",
    "<br>\n",
    " Epoch 1\n",
    " <br>\n",
    " loss: 2.6549 - categorical_accuracy: 0.1130 - val_loss: 2.4141 - val_categorical_accuracy: 0.1657\n",
    " <br>\n",
    " Epoch 2\n",
    " <br>\n",
    " loss: 2.1765 - categorical_accuracy: 0.2163 - val_loss: 2.1450 - val_categorical_accuracy: 0.2762\n",
    " <br>\n",
    " Epoch 3\n",
    " <br>\n",
    " loss: 1.8237 - categorical_accuracy: 0.3428 - val_loss: 1.9869 - val_categorical_accuracy: 0.3380\n",
    " <br>\n",
    " Epoch 4\n",
    " <br>\n",
    " loss: 1.5530 - categorical_accuracy: 0.4329 - val_loss: 1.9271 - val_categorical_accuracy: 0.3668\n",
    " <br>\n",
    " Epoch 5\n",
    " <br>\n",
    " loss: 1.3262 - categorical_accuracy: 0.5081 - val_loss: 1.9690 - val_categorical_accuracy: 0.3915\n",
    " <br>\n",
    " Epoch 6\n",
    " <br>\n",
    " loss: 1.2317 - categorical_accuracy: 0.5622 - val_loss: 1.8232 - val_categorical_accuracy: 0.4891"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "METRICS ON TEST DATASET PER CATEGORY:\n",
    "                           precision    recall  f1-score   support(number of examples per category in test)\n",
    "\n",
    "             alt.atheism       0.41      0.33      0.32       319\n",
    "           comp.graphics       0.66      0.51      0.63       389\n",
    " comp.os.ms-windows.misc       0.59      0.82      0.61       394\n",
    "comp.sys.ibm.pc.hardware       0.61      0.53      0.54       392\n",
    "   comp.sys.mac.hardware       0.52      0.54      0.51       385\n",
    "          comp.windows.x       0.81      0.58      0.60       395\n",
    "            misc.forsale       0.60      0.51      0.59       390\n",
    "               rec.autos       0.47      0.43      0.41       396\n",
    "         rec.motorcycles       0.41      0.42      0.44       398\n",
    "      rec.sport.baseball       0.52      0.71      0.61       397\n",
    "        rec.sport.hockey       0.33      0.22      0.23       399\n",
    "               sci.crypt       0.50      0.63      0.58       396\n",
    "         sci.electronics       0.41      0.61      0.57       393\n",
    "                 sci.med       0.63      0.68      0.61       396\n",
    "               sci.space       0.51      0.64      0.52       394\n",
    "  soc.religion.christian       0.47      0.72      0.53       398\n",
    "      talk.politics.guns       0.52      0.40      0.47       364\n",
    "   talk.politics.mideast       0.51      0.51      0.51       376\n",
    "      talk.politics.misc       0.58      0.22      0.33       310\n",
    "      talk.religion.misc       0.32      0.24      0.35       251\n",
    "\n",
    "             avg / total       0.51      0.51      0.49      7532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues faced : \n",
    "   &ensp; 1) Each epoch took almost 3 hours for execution with system configuration mentioned at the start. \n",
    "<br>\n",
    "   &ensp; 2) Ran the training for 6 epochs with batch size of 40 due to memory restrictions on the machine as high batch size was leading to memory issues.\n",
    "<br>\n",
    "   &ensp; 3) Did not perform hyper parameter tuning using cross validation/ grid search as the training took lot of time. \n",
    "# Future enhancements : \n",
    "   &ensp; 1) Run on GPU for more number of epochs with larger batch size to reduce over-fitting. \n",
    "<br>\n",
    "    &ensp; 2) Perform cross validation and grid search and chose the hyper parameters of learning rate and the vocab size that original vector can be shrinked to. \n",
    "<br>\n",
    "    &ensp; 3) Model selection by trying a multi-layer model and Convolutional network as to which will perform better. \n",
    "<br>\n",
    "   &ensp; 4) Find out if there are anyways to reduce the dimensionality. Applying PCA can work upon flattening the input vector which is of shape (None * sentencesize * 200), but I believe contextual information will be lost for training. So, need to research on dimensionality reduction techinques that can reduce the computational time exponentially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code for cross validation :  \n",
    "#wrap up the model into a base estimator class and use sklearns gridsearch cv \n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "Class OurEstimator(BaseEstimator):\n",
    "    \"\"\" our base esimtator\"\"\"\n",
    "    def __init__(self, x_train, vocal_len, y_train ,n_symbols , embedding_weights, input_length ,num_classes, learning_rate, activation):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.n_symbols = n_symbols\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.input_length = input_length\n",
    "        self.num_classes = num_classes \n",
    "        self.vocal_len = vocal_len\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "    def fit():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(output_dim= vocab_len, input_dim=self.n_symbols, mask_zero=True, weights=[self.embedding_weights], input_length=self.input_length))\n",
    "        model.add(LSTM(vocab_len))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(num_classes, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        print('Compiling the Model...')\n",
    "        adam = Adam(lr= 0.01)\n",
    "        model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        print(\"Train...\")\n",
    "        out_batch = NBatchLogger(display= batch_size)\n",
    "        y_train = np.array(y_train)\n",
    "        model.fit(x_train, y_train, batch_size = batch_size, epochs = n_epoch, shuffle = True , validation_split= 0.2,callbacks=[out_batch])\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    def predict():\n",
    "        y_pred = model.predict_proba(x_test_vec_wordembd)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "        \n",
    "\n",
    "tune_params = {'activation' = ['sigmoid', 'reLU'], learning_rate = np.arange(0.03,0.1 , 0.02)}\n",
    "gs = GridSearchCV(MeanClassifier(), tun_params,n_jobs = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References : \n",
    "[Microsoft research over generative vs discriminative models in deep learning](https://www.microsoft.com/en-us/research/publication/deep-discriminative-and-generative-models-for-pattern-recognition/)\n",
    "<br>\n",
    "[CS 224d : Deep learning for NLP](http://cs224d.stanford.edu/)\n",
    "<br>\n",
    "[Theano deep learning](http://deeplearning.net/tutorial/lstm.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
